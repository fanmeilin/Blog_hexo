# 20210126

# 1 决策树

一条信息的信息量大小和它的不确定性有直接关系

## 1.1 熵

### 1.1.1 信息熵

信息熵：判定信息的大小

信息熵的单位：比特

![image-20210126091622561](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210126091622561.png)

### 1.1.2 条件熵

其中log底数为2

![image-20210126093728857](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210126093728857.png)

信息获取量：通过特征字段作为节点分类获取了多少信息；



## 1.2 ID3算法

### 1.2.1 信息增益

信息增益：信息熵-条件熵；信息增益越大，则表示使用其属性进行分类决策后获得的“纯度提升”越大；

选择对大的信息增益作为分类节点

![image-20210126094613560](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210126094613560.png)

### 1.2.2 对比

id3是对于分类变量分析，因此需要连续变量离散化

![image-20210126102244168](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210126102244168.png)

## 1.3 C4.5

信息增益率：在自变量A条件下，目标变量D的信息增益/自变量A的信息熵

![image-20210126102502529](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210126102502529.png)

# 20210127

# 1 KNN

## 1.1 k近邻

最近邻域法：属于惰性算法

特点：不必事先创建全局的判别公式或者规则

## 1.2 k近邻的算法

1. 为了判别未知数据的类别，以所有已知的类别实例作为参考
2. 重要步骤:计算未知实例与所有已知实例的距离
3. 选择参考的实例参数k
4. 根据少数服从多数的原则，让未知的实例归类于k个最近邻样本中最多数的类别（投票法则）



## 1.3 距离的算法选择

### 1.3.1 欧氏距离

欧几里得度量（euclidean metric）（也称欧氏距离）是一个通常采用的距离定义，指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。在二维和三维空间中的欧氏距离就是两点之间的实际距离。

![img](https://bkimg.cdn.bcebos.com/formula/34df776ddfdd406a96fbafedf1e0fcf4.svg)

### 1.3.2 闵可夫斯基距离：

闵氏距离有时也指[时空间隔](https://baike.baidu.com/item/时空间隔/894418)

②设n维空间中有两点坐标x, y，p为常数，闵式距离定义为 [1] 

![img](https://bkimg.cdn.bcebos.com/formula/5461915cb8cdb80332251b27ecb23270.svg)

注意：

（1）闵氏距离与特征参数的量纲有关，有不同量纲的特征参数的闵氏距离常常是无意义的。

（2）闵氏距离没有考虑特征参数间的相关性，而马哈拉诺比斯距离解决了这个问题。

### 1.3.3 曼哈顿距离

两点在南北方向上的距离加上在东西方向上的距离，即d(i,j)=|xi-xj|+|yi-yj|。

### 1.3.4 对比说明

欧式距离使用得最多 **连续占比较大时使用欧式距离** 分类变量

**连续占比较小时使用曼哈顿距离**  虚拟变量

## 1.4 交叉验证

在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数据。而训练集和评估集则牵涉到下面的知识了。

因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation） [1] 。

***K-fold Cross Validation***

另外一种折中的办法叫做K折交叉验证，和LOOCV的不同在于，我们每次的测试集将不再只包含一个数据，而是多个，具体数目将根据K的选取决定。比如，如果K=5，那么我们利用五折交叉验证的步骤就是：

1.将所有数据集分成5份

2.不重复地每次取其中一份做测试集，用其他四份做训练集训练模型，之后计算该模型在测试集上的![[公式]](https://www.zhihu.com/equation?tex=MSE_i)

3.将5次的![[公式]](https://www.zhihu.com/equation?tex=MSE_i)取平均得到最后的MSE



![img](https://pic4.zhimg.com/80/v2-fcb843dd06c15a515d03a543864bbb77_720w.png)

## 1.5 ROC AUC ***

### 1.5.1 ROC曲线

![image-20210127114408570](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210127114408570.png)

受试者工作特征曲线

ROC曲线的横坐标是假阳性率（FPR）纵坐标是真阳性率（TPR）

假阳性率：在实际为0 被错误判定为1比例

真阳性率：在实际为1 被正确判定为1比例

![image-20210127114531940](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210127114531940.png)

![image-20210127114706179](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210127114706179.png)

![image-20210127114733950](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210127114733950.png)

### 1.5.2 AUC

AUC（Area Under Curve）被定义为[ROC曲线](https://baike.baidu.com/item/ROC曲线/775606)下与[坐标轴](https://baike.baidu.com/item/坐标轴/9763108)围成的[面积](https://baike.baidu.com/item/面积/100551)，显然这个面积的数值不会大于1。又由于[ROC曲线](https://baike.baidu.com/item/ROC曲线/775606)一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高;等于0.5时，则真实性最低，无应用价值。

**从AUC 判断分类器（预测模型）优劣的标准：**

- AUC = 1，是完美分类器。
- AUC = [0.85, 0.95], 效果很好
- AUC = [0.7, 0.85], 效果一般
- AUC = [0.5, 0.7],效果较低，但用于预测股票已经很不错了
- AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

# 2 案例分析

## 2.1 数据预处理

### 2.1.1 极差标准化

$$
x' = (x - min(X))/(max(X)-min(X))
$$



### 2.1.2 中心标准化

### 2.1.3 哑变量生成

### 2.1.4 代码

提取特征值

```python
features = data.loc[:,'income':'edueduclass']
features.head()
```

提取目标值

```python
label = data.Dated
label.head()
```

连续变量进行预处理，进行数据标准化，消除量纲不一致对各个变量产生得影响；

```python
# 数据预处理
from sklearn import preprocessing
min_max_sca = preprocessing.MinMaxScaler()
features_sca = min_max_sca.fit_transform(features)
```

## 2.2 切割数据集

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features_sca,label,test_size=0.2)
```

## 2.3 使用k近邻训练模型

```python
from sklearn.neighbors import KNeighborsClassifier
#模型训练
model = KNeighborsClassifier()
model.fit(X_train,y_train)
```

## 2.4 预测

```python
y_predict = model.predict(X_test)
```

## 2.5 评估训练结果

model.score（）：

```python
Signature: model.score(X, y, sample_weight=None)
Docstring:
Returns the mean accuracy on the given test data and labels.

In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.
```

metrics.classification_report（）

```python
Signature: metrics.classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)
Docstring:
Build a text report showing the main classification metrics

Read more in the :ref:`User Guide <classification_report>`.
```

```python
import sklearn.metrics as metrics
print(metrics.classification_report(y_test,y_predict))
#计算平均准确率
model.score(X_test,y_test)
```

## 2.6 找出最优参数

### 2.6.1 找出最优得参数 k值

```python
x =[]
y=[]
for item in range(1,70):
    modelk = KNeighborsClassifier(n_neighbors=item)
    modelk.fit(X_train,y_train)
    score = modelk.score(X_test,y_test)
    x.append(item)
    y.append(score)
   # print("当k的值为：",item,'score is',score)
```

### 2.6.2 交叉验证

当样本量较小时，适合采用交叉验证 评估模型的效果 需要设定用户搜索空间

十折交叉验证

```python
from sklearn.model_selection import ParameterGrid,GridSearchCV
grid = ParameterGrid({'n_neighbors':[range(1,15)]})
new_model = KNeighborsClassifier()
knn_cv = GridSearchCV(new_model,grid,cv=10,scoring='roc_auc')
knn_cv.fit(X_train,y_train)
```

 GridSearchCV(new_model,grid,cv=10,scoring='roc_auc')中cv=10表示十折，评估标准是roc_auc

### 2.6.3 获取AUC

获取测试集上的AUC的值

```python 
metrics.roc_auc_score(y_test,knn_cv.predict(X_test))
```

# 20210128

# 1 朴素贝叶斯

## 1.1 定义

分类模型，构造基础贝叶斯理论

朴素贝叶斯是基于贝叶斯定理和全概率公式，可以计算在自变量一定取值条件下，因变量的条件概率限制了自变量的取值类型（**分类变量**），并且自变量**相互独立**；

## 1.2 公式

### 1.2.1 全概率公式

![image-20210128105624694](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210128105624694.png)

### 1.2.2 贝叶斯公式

![image-20210128105642741](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210128105642741.png)

### 1.2.3 高斯模型

| 模型 |头文件| 说明                         |
| ---------- | --------------------------------------------- | ------------------------------------------------------------ |
| 高斯模型   | from sklearn.naive_bayes import GaussianNB    | 有些特征可能是连续型变量                                     |
| 伯努利模型 | from sklearn.naive_bayes import BernoulliNB   | 使用特征为全局特征，在伯努利模型中，每个特征的特征是布尔类型 |
|多项式模型|from sklearn.naive_bayes import MultinomialNB | 常用文本分类 特征是单词值出现的次数            |



## 1.3 拉普拉斯平滑系数

出现原因：在训练样本过程中，可能出现一些特征的缺失，某些特征出现的次数为0,以此方式计算时可能最终结果为0；
$$
p(x1,x2,x3,....|yi) = p(x1|yi)p(x2|yi)p(x3|yi)
$$

$$
pi = (Ni+a)/(N+a*m)
$$

a：平滑系数；m：特征值向量个数；a=1时称为拉普拉斯平滑

# 2 文本特征提取

 用于文档分类 垃圾邮件分类 新闻分类 文本分类是通过词是否存在或者词的重要性（概率）来表示

## 2.1 文档中词的出现

```python
from sklearn.feature_extraction.text import CountVectorizer
data2 = [
    'i love python',
    'life is short,i use python',
    'i dislike python'
]
cv = CountVectorizer()
cv.fit_transform(data2).toarray()
```

```python
array([[0, 0, 0, 1, 1, 0, 0],
       [0, 1, 1, 0, 1, 1, 1],
       [1, 0, 0, 0, 1, 0, 0]], dtype=int64)
```

```python
cv.get_feature_names()
结果为：['dislike', 'is', 'life', 'love', 'python', 'short', 'use']
```

## 2.2 文章中词的重要性

```python
from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(stop_words='english')
tv.fit_transform(data2).toarray()
```

```python
array([[0.        , 0.        , 0.861037  , 0.50854232, 0.        ,
        0.        ],
       [0.        , 0.54645401, 0.        , 0.32274454, 0.54645401,
        0.54645401],
       [0.861037  , 0.        , 0.        , 0.50854232, 0.        ,
        0.        ]])
```

```python
tv.vocabulary_
结果是 {'love': 2, 'python': 3, 'life': 1, 'short': 4, 'use': 5, 'dislike': 0}
tv.get_feature_names()
结果是 ['dislike', 'life', 'love', 'python', 'short', 'use']
```

## 2.3 关于中文文章进行分析的问题

### 2.3.1 结巴分词

```python 
import jieba
data3 = [
    '因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的',
    '但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。',
    '因此我们通常并不会把所有的数据集都拿来训练'
]
datalist=[]
for i in range(len(data3)):
    newlist=[]
    for j in jieba.cut(data3[i]):
        newlist.append(j)
    content = (' ').join(newlist)
    datalist.append(content)
ch_tf = TfidfVectorizer(stop_words=['的','（','）','，','。','了'])
ch_tf.fit_transform(datalist).toarray() 
```

```python 
array([[0.        , 0.        , 0.        , 0.        , 0.32927047,
        0.        , 0.32927047, 0.25041868, 0.        , 0.        ,
        0.25041868, 0.        , 0.        , 0.25041868, 0.32927047,
        0.58341732, 0.32927047, 0.19447244, 0.        , 0.        ],
       [0.        , 0.37766105, 0.37766105, 0.37766105, 0.        ,
        0.        , 0.        , 0.28722096, 0.        , 0.        ,
        0.28722096, 0.        , 0.28722096, 0.28722096, 0.        ,
        0.2230527 , 0.        , 0.2230527 , 0.37766105, 0.        ],
       [0.37072514, 0.        , 0.        , 0.        , 0.        ,
        0.37072514, 0.        , 0.        , 0.37072514, 0.37072514,
        0.        , 0.37072514, 0.28194602, 0.        , 0.        ,
        0.21895624, 0.        , 0.21895624, 0.        , 0.37072514]])
```

```python
ch_tf.get_feature_names()
结果是 ['不会',
 '之外',
 '令人满意',
 '但是',
 '因为',
 '因此',
 '实际',
 '对于',
 '我们',
 '所有',
 '拟合',
 '拿来',
 '数据',
 '程度',
 '结果',
 '训练',
 '还是',
 '通常',
 '那么',
 '集都']
```

# 20210129

# 1 神经网络

## 1.1 BP神经网络

![image-20210128233510841](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210128233510841.png)

## 

根据真是值，对于偏置和权重进行更新

![image-20210129000757438](C:\Users\86137\AppData\Roaming\Typora\typora-user-images\image-20210129000757438.png)

## 1.2 计算

神经元4：0.332

神经元5：0.525

神经元6：0.474

Err6 = 0.1311 Err4 = -0.0087 Err5 = -0.0065

学习率为：0.9

更新： 

权重更新：

w46 = -0.3 + 0.9\*0.1311\*0.332 = -0.261          w56 = -0.2 + 0.9\*0.1311*0.525 = 0.138

w14 = -0.2 + 0.9\*-0.0087\*1 = 0.192                 w15 = -0.3 + 0.9\*-0.0065* = -0.306

w24 = -0.4 + 0.9\*-0.0087\*0 = -0.4                    w25 = -0.1 + 0.9\*-0.0065\*0 = 0.1

w34 = -0. + 0.9\*-0.0087\*1 = -0.508                  w35 = 0.2 + 0.9\*-0.0065\*1 = 0.194

偏置项更新：

θ6 = 0.1+0.9*0.1311 = 0.218                              θ5 = 0.2+0.9\*-0.0065 = 0.194

θ4 = -0.4+0.9*-0.0087 = -0.408

# 2 案例分析

## 2.1 切分特征值和目标值

```python
feature = data.loc[:,'gender':]
print(feature.head())
label = data['churn']
print(label.head())
```

## 2.2 将数据集进行切分 

```python
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(feature,label)
```

#神经网络需要对数据进行极差标准化 是一个黑盒模型
#变量是连续变量，分类变量 一般数据处理要求进行极差标准化 分类变量需要转换为虚拟变量
#但是也可以特殊处理 多分类名义变量必须转化为寻变量 等级变量和二分类变量可以不转换当作连续变量处理

## 2.3 利用数据进行训练转换器

应用训练集x数据的固有属性，将其应用于训练集和测试集

```python 
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train)
#得到数据转化之后的数据
scaler_train_data = scaler.transform(X_train)
scaler_test_data = scaler.transform(X_test)
```

## 2.4 构建感知器模型

```python
#设置一个隐藏层 和10个神经元
from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(10,),activation='logistic')
mlp.fit(scaler_train_data,y_train)
```

## 2.5 预测模型

```python
y_predict = mlp.predict(scaler_test_data)
```

## 2.6 评估模型

```python 
import sklearn.metrics as metrics
print(metrics.classification_report(y_test,y_predict))
#混淆矩阵
print(metrics.confusion_matrix(y_test,y_predict,labels=[0,1]))
#平均准确率
mlp.score(scaler_test_data,y_test)
```

## 2.7 寻找最优参数

```python 
# 寻找最优参数
from sklearn.model_selection import GridSearchCV
param ={
    'hidden_layer_sizes':[(10,),(20,)],
    'activation':[ 'logistic', 'tanh', 'relu'],
    'alpha':[0.001,0.1,0.01]
}
newmlp = MLPClassifier(max_iter=1000)
#cv=5表示5折交叉验证
gsc = GridSearchCV(estimator=newmlp,param_grid=param,scoring='roc_auc',cv=5)
gsc.fit(scaler_train_data,y_train)
```

