---
title: praticalML第四章笔记
tags: [李沐,机器学习]
categories: [学习笔记,机器学习]
date: 2022-01-17 15:31:26
math: true
---

> 斯坦福2021秋季的实用机器学习上线啦~ :wink: 跟着沐神复习机器学习冲冲冲！ 记录一下笔记和重点，不一定很全哦~，只是记录自我感觉的重点。

# 第四章

## 4.1 模型评估

P来源于data采样得到模型之后得到预测，这个新的预测是更加关心的。

给定数据给定 超参数 已经获取到模型

- **Loss measures** how good the model in predicting the outcome in supervised learning 
-  Other metrics to evaluate the model performance 
  • Model specific: e.g. accuracy for classification, mAP for object detection 
  • Business specific: e.g. revenue, inference latency 
-  We select models by multiple metrics  
  • Just like how you choose cars

在监督学习中损失value是一个好的指标，除此之外，还有其他的精度比如acc，mAP，商业相关指标（营收增长，流量），我们会衡量多个指标来判断模型好坏。

### 4.1.1 常见分类指标

- 准确率Accuracy: # correct predictions / # examples  得到的y中有多少正确预测了	

    ```python
    sum(y==y_hat)/y.size
    ```

- 精度Precision: # True positive / # (True positive + False positive) 

  负类数据远大于正类，并且更关注正类时，acc就不适用了。

  precision对于预测具体真实类别（正例），哪些被正确预测了。判断为正例的置信度很大，也就是对正例判断越严谨精度越大。

    ```python
    sum((y_hat==1)&(y==1))/sum(y_hat==1)
    ```

- 召回率Recall: # True positive / # Positive examples 

  在所有的正类中有多少被预测出来了，也就是考虑覆盖性的问题，是否所有的正例都被预测出来了。

  ```python
  sum((y_hat==1)&(y==1))/sum(y==1)
  ```

- One metric that balances precision and recall  精度和召回率的权衡

    • **F1**: the harmonic mean of precision and recall: **2pr /( p + r)**

#### AUC-ROC

-  AUC，the area uder ROC curve，measures the chance a  model can distinguish classees.

- choose various $\theta$,predict as pos if $\hat{y} \geq \theta$ else neg

  实际生产当中，$ \theta $是需要选取调节的

  ROC曲线的

  x轴是 分母是负例样本 分子是预测成正例的负例样本数

  y轴是 正例的召回率 分母是所有的正类 分子是预测正确的正类样本数

  ![](https://picture.mulindya.com/pML4-1.png)

AUC是表示ROC下的面积，取值范围是[0,1]

![](https://picture.mulindya.com/pML4-2.png)

#### 商业指标

-  Optimize both revenue and customer experience 
  • Latency: ads should be shown to users at the same time as others   
  • ASN: average #ads shown in a page 
  • CTR: actual user click through rate 
  • ACP: average price advertiser pays per click 
-  revenue = #pageviews   ASN   CTR   ACP

Latency表示在多久在页面显示，一般控制在500ms之内，包括模型计算预测部署的时间

ASN 每一页投放的广告数量

CTR 上线之后点击频率

ACP 每一个广告点击，广告主的支付费用

#### 4.1.2 模型->商业评估

-  The key model metric is AUC 
-  A new model with increased AUC may harm business metrics, possible reasons: 
  • Lower estimated CTR   less ads displayed 
  • Lower real CTR because we trained and evaluated on past data 
  • Lower prices 
- Online experiment: deploy models to evaluate on real traffic data

实际上只看auc是不够的，需要各种权衡和模拟。

### 4.1.3 总结

-  We evaluate models with multiple metrics 
-  Model metrics evaluate model performance on examples 
  • E.g. accuracy, precision, recall, F1, AUC for classification models  
-  Business metrics measure how models impact the product

## 4.2 过拟合&欠拟合

### 4.2.1 训练误差 泛化误差

• **Training error**: model error on the training data 
• **Generalization error**: model error on new data

![](https://picture.mulindya.com/pML4-3.png)

### 4.2.2 原因

![](https://picture.mulindya.com/pML4-4.png)

数据复杂性 VS 模型复杂性

两者的复杂性越对等越好；

![](https://picture.mulindya.com/pML4-5.png)

数据简单 and 模型复杂就会导致过拟合

数据比模型更复杂，模型不能拟合数据，就会导致欠拟合。

### 4.2.3 模型复杂度

- The capacity of a set of function to fit data points 
- In ML, model complexity usually refers to: 
  • The number of learnable parameters  
  • The value range for those parameters 
-  It’s hard to compare between different types of ML models 
  • E.g. trees vs neural network 
-  More precisely measure of complexity: VC dimension 
  • VC dim for classification model: the maximum number of examples the model can shatter

模型复杂性可以描述为拟合各种函数的能力，很难比较模型之间的复杂度(比如树和神经网络)，通常参数越多，每层的参数范围越大，模型越复杂.

### 4.2.4 产生的影响

![](https://picture.mulindya.com/ParacticeML4-1-1.png)

固定数据，选取不同的复杂性的模型，可以看到模型越复杂越容易过拟合，越简单会导致欠拟合，最好的情况是泛化误差最小的时候，但是在最优解处也是存在过拟合的。

#### 具体例子

将买房的数据切分得到训练集和验证集，使用`scikit-learn`中的`DecisionTreeRegressor`（决策树回归器）树的深度为x轴，y为错误率。

![](https://picture.mulindya.com/ParacticeML4-1-2.png)



### 4.2.5 数据复杂度

-  Multiple factors matters

  • # of examples

  • # of features in each example

  • time/space structure

  • diversity

-  Again, hard to compare among very different data

  • E.g a char vs a pixel 

-  More precisely, Kolmogorov complexity 

  • A data is simple if it can be generated by a short program

多样性，时序结构，空间结构的复杂性；一般可以比较类似的数据集的复杂度；

### 4.2.6 Data VS Model

![](https://picture.mulindya.com/ParacticeML4-1-3.png)

随着数据的复杂性都会使得模型的泛化能力增加，误差减少。

蓝色是简单模型，绿色是复杂模型；在起始状态，复杂模型容易过拟合，所以泛化能力较差；但是数据越复杂复杂模型训练会比简单的模型效果更好。因此当数据比较简单时，使用简单模型就可以了，随着数据量变大，可以来匹配更复杂的模型这样泛化误差更小。

#### 泛化误差

-  Generalization error bound (an informal statement)

  {% raw %}
  $$
  |error \ on \ unseen \ data - training \ error| \le \sqrt{\frac{D}{N}(log(\frac{2N}{D}+1))}
  $$
  {% endraw %}

  • D: VC-dim, N: number of training examples

- Generalization error also depends on the training algorithm

  • Adding regularization can penalize complex models

  • Model trained with stochastic gradient methods generalizes better

泛化误差与训练误差之间的差距不超过某个值，这个值和训练算法有关，可以加入正则化项和惩罚模型，可以使用SGD的方法来训练。

### 4.2.7 模型选择

- Pick a model with a proper complexity for your data

  • Minimize the generalization error 

  • Also consider business metrics

-  Pick up a model family, then select proper hyper-parameters 调参

   • Trees: #trees, maximal depths 

  • Neural networks: architecture, depth (#layers), width (#hidden units), regularizations

需要选择和数据合适复杂度的模型，同时考虑商业指标。

### 4.2.8 总结

- 我们更关注泛化误差而不是训练误差；
- 复杂度：函数/信息量越大，复杂度越高；
- 模型选择：数据复杂度和模型复杂度的匹配。

## 4.3 模型验证

主要讲解了validate数据集和test数据集，评估泛化误差使用验证集，最后使用测试集。

数据足够多，可以选取一半的数据作为验证；

数据不多可以选取10%，20%作为验证集；

### 4.3.1 非独立同分布数据切分

一般随机分数据不可行，除非数据独立同分布。

- Random data splitting may lead to underestimate of generalization error

- Sequential data 买房，股票信息都有时序信息的 

​	• e.g. house sales, stock prices

​	• Valid set should not overlap with train set in time 重复信息 比如人脸识别，在分数据的时候同一张脸出现在训练集和测试集之中，这是不符合规范的。

- Examples are highly clustered 类别数量不均衡

  • e.g. photos of the same person, clips of the same video

   • Split clusters instead of examples

-  Highly imbalanced label classes

  • Sample more from minor classes

需要保证验证集样本在训练集之后，同时保证数据集没有重复的（可以首先在组信息分割）。

#### 反向实例

![](https://picture.mulindya.com/ParacticeML4-3-1.png)



第一幅图是前面一半的时间的样本数据训练，后面的时间样本数据预测；

第二幅图是随机挑选数据来训练和预测。

通过决策树查看训练，分析过拟合的出现；比较两者，在随机挑选得数据中过拟合发生得晚一些，它可以获取更加整体得细节，前者过度拟合了局部信息。

在线性回归上更加明显，使用顺序的数据在验证集上偏差很大。

### 4.3.2 K-fold Cross Validation

![](https://picture.mulindya.com/ParacticeML4-3-2.png)

当数据不充足时可以使用K折交叉验证。把数据切成K份，然后选取k-1份训练另外一份来验证，再去k个验证误差来做平均得到验证集误差。一般K选择5或则10。比如K为5时，80%数据训练，20%数据验证，重复5次得到验证集误差。数据比较少的时候可以选择10，也就是在90%数据训练，10%数据验证。代价是重复更多的次数。

### 4.3.3 Bug

• If your ML model performance is too good to be true, very likely there is a bug, and contaminated valid set is the #1 reason

- Valid set has examples from train set

  • duplicated examples from original set

  • Often happens when integrating multiple datasets

  ​	• Scrape images from search engine to evaluate models trained on ImageNet 

-  Information leaking from train set to valid set

  • Often happens for non I.I.D data

  • use future to predict past, see a person’s face before

  • Excessive use of valid set for hyper param tuning is cheating

表现效果特别好，有时可能出现了bug；需要检查一下：

1. 验证集和训练集可能会有重叠；
2. 在数据融合时出现了数据泄露；

### 4.3.4 总结

-  The test data is used once to evaluate your model

- One can hold out a validation set from the training data to estimate the test data

   • You can use valid set multiple times for model selections and hyper param tuning

  • Validation data should be close to the test data

  • Improper valid set is a common mistake that lead to over estimate of the model performance

验证集要尽量的选取真实数据。
